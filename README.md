# BunkrIndex â€” Self-Hosted Album Discovery Site

A fully automated, free, GitHub Pages-hosted searchable index of Bunkr albums.  
**No servers. No costs. Auto-updates every 6 hours.**

---

## ğŸ—‚ Project Structure

```
bunkr-index/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scrape.yml      â† GitHub Actions automation
â”œâ”€â”€ scraper.py              â† Metadata-only scraper (no downloads)
â”œâ”€â”€ generate_rss.py         â† RSS feed generator
â”œâ”€â”€ requirements.txt        â† Python deps
â”œâ”€â”€ albums.json             â† Auto-generated index (committed by bot)
â”œâ”€â”€ feed.xml                â† RSS feed (auto-generated)
â””â”€â”€ index.html              â† Static frontend (served by GitHub Pages)
```

---

## ğŸš€ Deployment (5 minutes)

### Step 1 â€” Fork / create repo

1. Create a new **public** GitHub repo (e.g. `bunkr-index`).
2. Upload all files from this project into it.

### Step 2 â€” Enable GitHub Pages

1. Go to **Settings â†’ Pages**
2. Source: **Deploy from a branch**
3. Branch: `main` / `(root)`
4. Click **Save**

Your site will be live at:  
`https://YOUR_USERNAME.github.io/bunkr-index/`

### Step 3 â€” Update the RSS site URL

Edit `generate_rss.py` and change:
```python
SITE_URL = "https://YOUR_USERNAME.github.io/bunkr-index"
```

### Step 4 â€” Trigger the first scrape

1. Go to **Actions â†’ Scrape & Index Albums**
2. Click **Run workflow**
3. Wait ~2 minutes for it to complete
4. Reload your GitHub Pages URL â€” albums will appear!

### Step 5 â€” Automatic updates

The workflow runs automatically every 6 hours. No action needed.  
You can also manually trigger it anytime from the Actions tab.

---

## âš™ï¸ Configuration

| Env Variable | Default | Description |
|---|---|---|
| `MAX_ALBUMS` | `500` | Max new albums to index per run |
| `REQUEST_DELAY` | `1.5` | Seconds between HTTP requests |

Set these in the workflow dispatch inputs or repo **Settings â†’ Secrets and variables â†’ Actions â†’ Variables**.

---

## ğŸ” Features

### Frontend
- âš¡ Instant client-side search (Lunr.js full-text with fuzzy matching)
- ğŸ¨ Dark mode only â€” premium design with animated cards
- ğŸ“± Responsive grid (works on mobile)
- âˆ Infinite scroll â€” loads 60 cards at a time
- ğŸ”¢ Filter by file count range (1â€“9, 10â€“49, 50â€“199, 200+)
- ğŸ–¼ Filter by thumbnail presence
- â†•ï¸ Sort by date, file count, or title
- âŒ¨ï¸ Press `/` to focus the search bar instantly
- ğŸ“¡ RSS feed (`/feed.xml`) for latest 50 albums

### Scraper
- Tries the unofficial Bunkr API first, falls back to HTML scraping
- Deduplicates across runs â€” never re-indexes known albums
- Enriches albums without thumbnails via detail page scraping
- Graceful retry on network errors (3 attempts with backoff)
- Stores only metadata â€” **zero file downloads**

---

## ğŸ“Š How the Index Grows

| Run | New Albums Added |
|-----|----------------|
| First | Up to 500 |
| Each subsequent | New albums since last run |
| After 1 week | 500â€“3,500+ total |

The scraper is conservative with rate limiting (1.5s between requests) to avoid bans.

---

## ğŸ›  Enhancements You Can Add

### Tag / category filtering
Parse album titles to auto-detect categories and add filter pills.

### Better discovery
Seed with known album IDs from external lists, then let the scraper expand from there.

### Sitemap
Add a `generate_sitemap.py` that creates `sitemap.xml` for Google indexing.

### Dark/light mode toggle
Add a CSS `[data-theme=light]` override and a toggle button.

### Album detail pages
Generate static `a/ALBUM_ID.html` pages for each album (better SEO).

---

## ğŸ“œ Legal

This project indexes **only publicly available metadata** (titles, file counts, thumbnail URLs that are already publicly visible). No files are downloaded. This is equivalent to a search engine index.

---

## ğŸ¤ Contributing

PRs welcome! Key areas to improve:
- Better Bunkr API reverse engineering
- Additional scraping fallbacks
- SEO improvements (structured data, sitemaps)
